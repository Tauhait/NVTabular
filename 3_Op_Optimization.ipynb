{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44057528",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68f9e9",
   "metadata": {},
   "source": [
    "# Op Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a33c9",
   "metadata": {},
   "source": [
    "Now that our pipeline is working well, let's learn to scale it with multiple GPUs.\n",
    "\n",
    "<b>Learning Objectives</b>:\n",
    "* Learn how to use a LocalCUDACluster to utilize multiple GPUs.\n",
    "* Learn how to [Rename](https://github.com/NVIDIA/NVTabular/blob/main/nvtabular/ops/rename.py) a column.\n",
    "* Learn how to export NVTabular to Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b044c",
   "metadata": {},
   "source": [
    "### 1. NVTabular with Multi-GPU support by LocalCUDACluster\n",
    "\n",
    "So far, we have used the simplest way to apply an NVTabular workflow which utilizes only a single GPU. NVTabular can easily be scaled to multiple GPUs by initializing a [LocalCUDACluster](https://dask-cuda.readthedocs.io/en/latest/ucx.html?highlight=localcudacluster#localcudacluster) first. We will begin by importing all of the libraries we need for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5fbe932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "\n",
    "import cudf\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "import numpy as np\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "from nvtabular.utils import device_mem_size, get_rmm_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f1c22",
   "metadata": {},
   "source": [
    "If it is not running already, we highly recommend setting up a terminal on the side to monitor our GPUs with the following command:\n",
    "\n",
    "`watch -n0.1 nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4044d5a",
   "metadata": {},
   "source": [
    "With this command, we can see that have 4 GPUs. So far, we've only been using one GPU (`0`), which is using the most memory. The others are barely using anything. Let's put them to work!\n",
    "\n",
    "To do this, we will be using a [LocalCUDACluster](https://dask-cuda.readthedocs.io/en/latest/ucx.html?highlight=localcudacluster#localcudacluster). Here are some of the key parameters:\n",
    "\n",
    "* `protocol`: Protocol to use for communication.\n",
    "  * `tcp` is the default communication.\n",
    "  * `ucx` enables to use NVIDIA's [NVLink](https://www.nvidia.com/en-us/data-center/nvlink/) technology, where GPU's can directly communicate with each other and achieves higher speed-ups. It requires `ucx`, `enable_tcp_over_ucx=True` and `enable_nvlink=True`.\n",
    "* `CUDA_VISIBLE_DEVICES`: Defines visible GPUs devices to the LocalCUDACluster\n",
    " * e.g. `0,1,3` for GPU 0, 1 and 3.\n",
    "* `local_directory`: Defines the directory to buffer data.\n",
    "* `device_memory_limit` : Reduce the memory limit for workers in your cluster. \n",
    "  * This setting may need to be much lower than the actual memory capacity of your device.\n",
    "  \n",
    "Rather than manually entering our memory limit, NVTabular has a number of [utils](https://github.com/NVIDIA/NVTabular/blob/main/nvtabular/utils.py) to help us figure out our capacity programmatically. Let's use 90% of our total capacity. If we use all 100%, there will be no room for anything else, which can lead to memory errors.\n",
    "\n",
    "**TODO**: We've created four variables to use as parameters to `LocalCUDACluster`:\n",
    "* device_memory_limit\n",
    "* temporary_data_directory\n",
    "* protocol\n",
    "* visible_devices\n",
    "\n",
    "Match one of each to the FIXMEs below.\n",
    "\n",
    "**Note**: If you need to rerun this cell, please restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31b94dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:36212</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>186.82 GiB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:36212' processes=4 threads=4, memory=186.82 GiB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a fraction of capacity to prevent memory errors\n",
    "device_memory_limit = device_mem_size(kind=\"total\") * .9 \n",
    "temporary_data_directory = '/tmp/'\n",
    "protocol = \"tcp\"             # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1,2,3\"  # Select devices to place workers\n",
    "\n",
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "cluster = LocalCUDACluster(\n",
    "    protocol = protocol,\n",
    "    CUDA_VISIBLE_DEVICES = visible_devices,\n",
    "    local_directory = temporary_data_directory,\n",
    "    device_memory_limit = device_memory_limit\n",
    ")\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631f752",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a fraction of capacity to prevent memory errors\n",
    "device_memory_limit = device_mem_size(kind=\"total\") * .9 \n",
    "temporary_data_directory = '/tmp/'\n",
    "protocol = \"tcp\"             # \"tcp\" or \"ucx\"\n",
    "visible_devices = \"0,1,2,3\"  # Select devices to place workers\n",
    "\n",
    "# Deploy a Single-Machine Multi-GPU Cluster\n",
    "cluster = LocalCUDACluster(\n",
    "    protocol = protocol,\n",
    "    CUDA_VISIBLE_DEVICES = visible_devices,\n",
    "    local_directory = temporary_data_directory,\n",
    "    device_memory_limit = device_memory_limit\n",
    ")\n",
    "\n",
    "# Create the distributed client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2c393",
   "metadata": {},
   "source": [
    "#### Initializing Memory Pools\n",
    "\n",
    "Since allocating memory is often a performance bottleneck, it is usually a good idea [to initialize](https://docs.rapids.ai/api/rmm/stable/basics.html) a memory pool on each of our workers. When using a distributed cluster, we must use the `client.run` utility to make sure a function is executed on all available workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb35259e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:33075': None,\n",
       " 'tcp://127.0.0.1:34696': None,\n",
       " 'tcp://127.0.0.1:40218': None,\n",
       " 'tcp://127.0.0.1:44853': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RMM pool on ALL workers\n",
    "def _rmm_pool():\n",
    "    rmm.reinitialize()\n",
    "client.run(_rmm_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123bb82",
   "metadata": {},
   "source": [
    "**Done! Congrats, you are using NVTabular with multi-GPU support, now.**\n",
    "\n",
    "That's it. After the LocalCUDACluster is initialized, we can use NVTabular as usual, but it will be executed on multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f1da4b",
   "metadata": {},
   "source": [
    "### Rename\n",
    "\n",
    "Let's put these GPUs to work with an NVTabular workflow. In our dataset, we have a column called `HourlyWindDirection`. It's in degrees with 0 degree and 360 degrees as true north."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9b7295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HourlyWindSpeed_difference_lag_1</th>\n",
       "      <th>HourlyWindDirection</th>\n",
       "      <th>HourlyRelativeHumidity</th>\n",
       "      <th>HourlyDewPointTemperature</th>\n",
       "      <th>HourlyDryBulbTemperature</th>\n",
       "      <th>HourlyWetBulbTemperature</th>\n",
       "      <th>HourlyWindSpeed_difference_lag_1_filled</th>\n",
       "      <th>HourlyWindDirection_filled</th>\n",
       "      <th>HourlyRelativeHumidity_filled</th>\n",
       "      <th>HourlyDewPointTemperature_filled</th>\n",
       "      <th>HourlyDryBulbTemperature_filled</th>\n",
       "      <th>HourlyWetBulbTemperature_filled</th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>0.497725</td>\n",
       "      <td>0.159465</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>72058700184</td>\n",
       "      <td>2012-04-22T11:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>0.974457</td>\n",
       "      <td>0.159465</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>72058700184</td>\n",
       "      <td>2011-06-05T23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>0.616908</td>\n",
       "      <td>0.159465</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>72058700184</td>\n",
       "      <td>2012-04-22T12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>12.222222</td>\n",
       "      <td>0.736091</td>\n",
       "      <td>0.159465</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>72058700184</td>\n",
       "      <td>2012-04-22T12:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>1.034049</td>\n",
       "      <td>0.159465</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>72058700184</td>\n",
       "      <td>2011-06-05T23:35:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HourlyWindSpeed_difference_lag_1  HourlyWindDirection  \\\n",
       "0                              -5.0                320.0   \n",
       "1                               0.0                  0.0   \n",
       "2                               6.0                310.0   \n",
       "3                              -5.0                310.0   \n",
       "4                               8.0                240.0   \n",
       "\n",
       "   HourlyRelativeHumidity  HourlyDewPointTemperature  \\\n",
       "0                    50.0                  12.222222   \n",
       "1                    70.0                  21.111111   \n",
       "2                    44.0                  11.111111   \n",
       "3                    44.0                  12.222222   \n",
       "4                    66.0                  21.111111   \n",
       "\n",
       "   HourlyDryBulbTemperature  HourlyWetBulbTemperature  \\\n",
       "0                  0.497725                  0.159465   \n",
       "1                  0.974457                  0.159465   \n",
       "2                  0.616908                  0.159465   \n",
       "3                  0.736091                  0.159465   \n",
       "4                  1.034049                  0.159465   \n",
       "\n",
       "   HourlyWindSpeed_difference_lag_1_filled  HourlyWindDirection_filled  \\\n",
       "0                                    False                       False   \n",
       "1                                    False                       False   \n",
       "2                                    False                       False   \n",
       "3                                    False                       False   \n",
       "4                                    False                       False   \n",
       "\n",
       "   HourlyRelativeHumidity_filled  HourlyDewPointTemperature_filled  \\\n",
       "0                          False                             False   \n",
       "1                          False                             False   \n",
       "2                          False                             False   \n",
       "3                          False                             False   \n",
       "4                          False                             False   \n",
       "\n",
       "   HourlyDryBulbTemperature_filled  HourlyWetBulbTemperature_filled  \\\n",
       "0                            False                             True   \n",
       "1                            False                             True   \n",
       "2                            False                             True   \n",
       "3                            False                             True   \n",
       "4                            False                             True   \n",
       "\n",
       "       STATION                 DATE  \n",
       "0  72058700184  2012-04-22T11:35:00  \n",
       "1  72058700184  2011-06-05T23:15:00  \n",
       "2  72058700184  2012-04-22T12:00:00  \n",
       "3  72058700184  2012-04-22T12:15:00  \n",
       "4  72058700184  2011-06-05T23:35:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cudf.read_parquet(\"data/transformed_out/*.parquet\")\n",
    "columns = df.columns.to_list()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba1f43",
   "metadata": {},
   "source": [
    "With degrees like this, 350 degrees and 10 degrees seem far away, when physically, they're close. Let's make a pipeline to convert our `HourlyWindDirection` into Latitude and Longitude components.\n",
    "\n",
    "Since our output cannot have two columns with the same name, we will use the [Rename](https://github.com/NVIDIA/NVTabular/blob/main/nvtabular/ops/rename.py) Op to make this possible. We will add a `postfix` to define which output is the latitude component and which output is the longitude component.\n",
    "\n",
    "**TODO**: We've provided an example of `Rename` below for the latitude component. Replace the FIXMEs below to add a `_long` tag to the longitude component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36932564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lat_speed(col):\n",
    "    return np.cos(np.radians(col))\n",
    "\n",
    "\n",
    "def long_speed(col):\n",
    "    return np.sin(np.radians(col))\n",
    "\n",
    "\n",
    "wind_lat = (\n",
    "    [\"HourlyWindDirection\"]\n",
    "    >> nvt.ops.LambdaOp(lat_speed)\n",
    "    >> nvt.ops.Rename(postfix=\"_lat\")\n",
    ")\n",
    "wind_long = (\n",
    "    [\"HourlyWindDirection\"]\n",
    "    >> nvt.ops.LambdaOp(long_speed)\n",
    "    >> nvt.ops.Rename(postfix=\"_long\")\n",
    ")\n",
    "\n",
    "# Combine all columns\n",
    "all_cols = columns + wind_lat + wind_long\n",
    "\n",
    "# Define workflow\n",
    "files = glob.glob(\"data/transformed_out/*.parquet\")\n",
    "workflow = nvt.Workflow(all_cols)\n",
    "dataset1 = nvt.Dataset(files, engine=\"parquet\")\n",
    "\n",
    "files = glob.glob(\"data/parquet_out/*.parquet\")\n",
    "workflow = nvt.Workflow(all_cols)\n",
    "dataset2 = nvt.Dataset(files, engine=\"parquet\")\n",
    "\n",
    "# files = glob.glob(\"data/renamed_out/*.parquet\")\n",
    "# workflow = nvt.Workflow(all_cols)\n",
    "# dataset3 = nvt.Dataset(files, engine=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc12a171",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lat_speed(col):\n",
    "    return np.cos(np.radians(col))\n",
    "\n",
    "\n",
    "def long_speed(col):\n",
    "    return np.sin(np.radians(col))\n",
    "\n",
    "\n",
    "wind_lat = (\n",
    "    [\"HourlyWindDirection\"]\n",
    "    >> nvt.ops.LambdaOp(lat_speed)\n",
    "    >> nvt.ops.Rename(postfix=\"_lat\")\n",
    ")\n",
    "wind_long = (\n",
    "    [\"HourlyWindDirection\"]\n",
    "    >> nvt.ops.LambdaOp(long_speed)\n",
    "    >> nvt.ops.Rename(postfix=\"_long\")\n",
    ")\n",
    "\n",
    "# Combine all columns\n",
    "all_cols = columns + wind_lat + wind_long\n",
    "\n",
    "# Define workflow\n",
    "files = glob.glob(\"data/transformed_out/*.parquet\")\n",
    "workflow = nvt.Workflow(all_cols)\n",
    "dataset = nvt.Dataset(files, engine=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22163c12",
   "metadata": {},
   "source": [
    "Let's verify that the pipeline is setup correctly. Confirm the following:\n",
    "* There is a `HourlyWindDirection_lat` column\n",
    "* There is a `HourlyWindDirection_long` column\n",
    "* The above two columns have different values\n",
    "* **All GPUs are used when running the workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d2846c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/renamed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15a14d3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following columns were not found in the dataset {'HourlyRelativeHumidity_filled', 'HourlyWindDirection_filled', 'HourlyWetBulbTemperature_filled', 'HourlyDryBulbTemperature_filled', 'HourlyDewPointTemperature_filled', 'HourlyWindSpeed_difference_lag_1_filled', 'HourlyWindSpeed_difference_lag_1'}\nThe following columns were found Index(['STATION', 'DATE', 'HourlyDewPointTemperature',\n       'HourlyDryBulbTemperature', 'HourlyPrecipitation',\n       'HourlyRelativeHumidity', 'HourlyWetBulbTemperature',\n       'HourlyWindDirection', 'HourlyWindSpeed'],\n      dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f905ef553dfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_out1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/renamed_out/*.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m workflow.transform(dataset2).to_parquet(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data/renamed_out/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_files_per_proc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/nvtabular/workflow.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_worker_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ddf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         return Dataset(\n\u001b[1;32m     96\u001b[0m             \u001b[0m_transform_ddf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/nvtabular/io/dataset.py\u001b[0m in \u001b[0;36mto_ddf\u001b[0;34m(self, columns, shuffle, seed)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \"\"\"\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# Use DatasetEngine to create ddf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mddf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_ddf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# Shuffle the partitions of ddf (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/nvtabular/io/parquet.py\u001b[0m in \u001b[0;36mto_ddf\u001b[0;34m(self, columns, cpu)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mgather_statistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0msplit_row_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_groups_per_part\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         )\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/dask_cudf/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, split_row_groups, row_groups_per_part, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0msplit_row_groups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_row_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCudfEngine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/io/parquet/core.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, gather_statistics, split_row_groups, read_from_paths, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;31m# Modify `meta` dataframe accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     meta, index, columns = set_index_columns(\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_in_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_index_allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mNONE_LABEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/dask/dataframe/io/parquet/core.py\u001b[0m in \u001b[0;36mset_index_columns\u001b[0;34m(meta, index, columns, index_in_columns, auto_index_allowed)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;34m\"The following columns were not found in the dataset %s\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;34m\"The following columns were found %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m         )\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The following columns were not found in the dataset {'HourlyRelativeHumidity_filled', 'HourlyWindDirection_filled', 'HourlyWetBulbTemperature_filled', 'HourlyDryBulbTemperature_filled', 'HourlyDewPointTemperature_filled', 'HourlyWindSpeed_difference_lag_1_filled', 'HourlyWindSpeed_difference_lag_1'}\nThe following columns were found Index(['STATION', 'DATE', 'HourlyDewPointTemperature',\n       'HourlyDryBulbTemperature', 'HourlyPrecipitation',\n       'HourlyRelativeHumidity', 'HourlyWetBulbTemperature',\n       'HourlyWindDirection', 'HourlyWindSpeed'],\n      dtype='object')"
     ]
    }
   ],
   "source": [
    "workflow.transform(dataset1).to_parquet(\n",
    "    output_path=\"data/renamed_out/\", out_files_per_proc=4\n",
    ")\n",
    "df_out1 = cudf.read_parquet(\"data/renamed_out/*.parquet\")\n",
    "\n",
    "workflow.transform(dataset2).to_parquet(\n",
    "    output_path=\"data/renamed_out/\", out_files_per_proc=4\n",
    ")\n",
    "df_out2 = cudf.read_parquet(\"data/renamed_out/*.parquet\")\n",
    "\n",
    "# workflow.transform(dataset3).to_parquet(\n",
    "#     output_path=\"data/renamed_out/\", out_files_per_proc=4\n",
    "# )\n",
    "# df_out3 = cudf.read_parquet(\"data/renamed_out/*.parquet\")\n",
    "\n",
    "df_out = df_out1 + df_out2\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601cf12",
   "metadata": {},
   "source": [
    "Congratulations, all done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d5ecc",
   "metadata": {},
   "source": [
    "In order to shut down our LocalCUDACluster, we can use the `close` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34e6f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca701ac9",
   "metadata": {},
   "source": [
    "### Dask Integration and Other Tools\n",
    "\n",
    "NVTabular is tightly integrated with [Dask-CuDF](https://github.com/rapidsai/dask-cudf), and we can convert a NVTabular Dataset to a Dask DataFrame. While we're at it, let's take a closer look at [NVTabular datasets](https://nvidia.github.io/NVTabular/main/api/dataset.html#).\n",
    "\n",
    "`??nvt.Dataset`\n",
    "\n",
    "```\n",
    " Parameters\n",
    "    -----------\n",
    "    path_or_source : str, list of str, or <dask.dataframe|cudf|pd>.DataFrame\n",
    "        Dataset path (or list of paths), or a DataFrame. If string,\n",
    "        should specify a specific file or directory path. If this is a\n",
    "        directory path, the directory structure must be flat (nested\n",
    "        directories are not yet supported).\n",
    "    engine : str or DatasetEngine\n",
    "        DatasetEngine object or string identifier of engine. Current\n",
    "        string options include: (\"parquet\", \"csv\", \"avro\"). This argument\n",
    "        is ignored if path_or_source is a DataFrame type.\n",
    "    part_size : str or int\n",
    "        Desired size (in bytes) of each Dask partition.\n",
    "        If None, part_mem_fraction will be used to calculate the\n",
    "        partition size.  Note that the underlying engine may allow\n",
    "        other custom kwargs to override this argument. This argument\n",
    "        is ignored if path_or_source is a DataFrame type.\n",
    "    part_mem_fraction : float (default 0.125)\n",
    "        Fractional size of desired Dask partitions (relative\n",
    "        to GPU memory capacity). Ignored if part_size is passed\n",
    "        directly. Note that the underlying engine may allow other\n",
    "        custom kwargs to override this argument. This argument\n",
    "        is ignored if path_or_source is a DataFrame type.\n",
    "    storage_options: None or dict\n",
    "        Further parameters to pass to the bytes backend. This argument\n",
    "        is ignored if path_or_source is a DataFrame type.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d643733",
   "metadata": {},
   "source": [
    "First, `path_or_source` defines our dataset source. For example, we can directly initialize a `nvt.Dataset` from a dask.dataframe.DataFrame or cudf.DataFrame, as below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61e3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates 1 partition\n",
    "df = cudf.DataFrame({'col1': [0,1,2,3,4], 'col2': ['a', 'b', 'c', 'd', 'e']})\n",
    "dataset = nvt.Dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12336f78",
   "metadata": {},
   "source": [
    "If we read data from disk, `path_or_source` is a string or list of strings for dataset path. The parameter `engine` defines the filetype and nvt.Dataset supports `parquet`, `csv` and `avro` file formats.\n",
    "\n",
    "**The parameters `part_size` or `part_mem_fraction` are important when we read data from disk. They define the size of the chunks we read in memory.** \n",
    "\n",
    "Only one of the parameters is used. If `part_size` is defined, then `part_mem_fraction` is ignored. By default, both parquet and csv-based data will be converted to a Dask-DataFrame collection with a maximum partition size of roughly 12.5 percent of the total memory on a single device.  The partition size can be changed to a different fraction of total memory on a single device with the `part_mem_fraction` argument.\n",
    "\n",
    "* `part_size` defines the size of each Dask partition in bytes. A good rule of thumb is `128MB` or `256MB`.\n",
    "* `part_mem_fraction` defines fractional size of desired Dask partitions relative to GPU memory.\n",
    "\n",
    "Example:\n",
    "* if `part_size='100MB'`, then each Dask partition is of 100MB (or smaller)\n",
    "* if `part_mem_fraction=0.1`, then 10% of the single GPU memory (or smaller) is used for the Dask partitions\n",
    "\n",
    "**TODO**: Use [glob](https://docs.python.org/3/library/glob.html) to find all of the `.parquet` files in either `data/parquet_out`, `data/renamed_out`, `data/transformed_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc77c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/transformed_out/2.919c533fafbf446cb635196a8e3165d8.parquet', 'data/transformed_out/3.a92bcfaf689040c6969d62a294d582a1.parquet', 'data/transformed_out/1.edfc22382da74215ab591337ffe3f6a8.parquet', 'data/transformed_out/0.7f1090bc95984029ade469df888e79a7.parquet']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"data/transformed_out/*.parquet\", recursive=True)\n",
    "print(files)\n",
    "# cudf.read_parquet(files).to_parquet(\"data/final_parquet\")\n",
    "dataset = nvt.Dataset(files, engine='parquet', part_size='100MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b34412",
   "metadata": {},
   "source": [
    "We can then convert this dataset to a Dask DataFrame using `to_ddf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df448230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HourlyWindSpeed_difference_lag_1</th>\n",
       "      <th>HourlyWindDirection</th>\n",
       "      <th>HourlyRelativeHumidity</th>\n",
       "      <th>HourlyDewPointTemperature</th>\n",
       "      <th>HourlyDryBulbTemperature</th>\n",
       "      <th>HourlyWetBulbTemperature</th>\n",
       "      <th>HourlyWindSpeed_difference_lag_1_filled</th>\n",
       "      <th>HourlyWindDirection_filled</th>\n",
       "      <th>HourlyRelativeHumidity_filled</th>\n",
       "      <th>HourlyDewPointTemperature_filled</th>\n",
       "      <th>HourlyDryBulbTemperature_filled</th>\n",
       "      <th>HourlyWetBulbTemperature_filled</th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 4 tasks</div>"
      ],
      "text/plain": [
       "<dask_cudf.DataFrame | 4 tasks | 4 npartitions>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dataset.to_ddf()\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa515837",
   "metadata": {},
   "source": [
    "We can check the memory usage per columns with [memory_usage](https://docs.rapids.ai/api/cudf/legacy/api.html#cudf.core.dataframe.DataFrame.memory_usage) and `memory_usage_per_partition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59cf3571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask_cudf.Series | 17 tasks | 1 npartitions>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee29782d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=4\n",
       "    int64\n",
       "      ...\n",
       "      ...\n",
       "      ...\n",
       "      ...\n",
       "dtype: int64\n",
       "Dask Name: total_mem_usage, 8 tasks"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.memory_usage_per_partition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2a170",
   "metadata": {},
   "source": [
    "However, since Dask is lazy, and nothing is computed yet, we're only given information about the graph nodes. To force a result, we can use `compute`. The memory usage is in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e09670ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE                                       11171484\n",
       "HourlyWindSpeed_difference_lag_1            3885728\n",
       "HourlyDryBulbTemperature                    1942864\n",
       "HourlyWindSpeed_difference_lag_1_filled      485716\n",
       "HourlyRelativeHumidity                      3885728\n",
       "HourlyWindDirection                         3885728\n",
       "HourlyDryBulbTemperature_filled              485716\n",
       "Index                                             0\n",
       "HourlyDewPointTemperature_filled             485716\n",
       "HourlyWetBulbTemperature_filled              485716\n",
       "HourlyWindDirection_filled                   485716\n",
       "HourlyDewPointTemperature                   3885728\n",
       "HourlyRelativeHumidity_filled                485716\n",
       "STATION                                     3885728\n",
       "HourlyWetBulbTemperature                    1942864\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.memory_usage().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0af99e",
   "metadata": {},
   "source": [
    "Congratulations on getting through this set of courses! If you would like to learn more about NVTabular and tools for Retail Big Data, these notebooks were created with help from the [NVIDIA Merlin](https://developer.nvidia.com/nvidia-merlin) team. Merlin is a framework used to handle industry scale Recommender Systems.\n",
    "\n",
    "There are four main components:\n",
    "\n",
    "<center><img src='https://developer.nvidia.com/sites/default/files/akamai/merlin/recommender-systems-dev-web-850.svg' width='80%'></center>\n",
    "\n",
    "* [NVTabular](https://nvidia.github.io/NVTabular/main/Introduction.html): Feature engineering and preprocessing library designed to quickly and easily manipulate terabytes of tabular data\n",
    "* NVTabular dataloader: Highly optimized dataloaders to accelerate TensorFlow and PyTorch pipelines\n",
    "* [HugeCTR](https://github.com/NVIDIA/HugeCTR): Highly efficient Python and C++ GPU framework and reference design dedicated for recommendation workload training\n",
    "* [Triton](https://developer.nvidia.com/nvidia-triton-inference-server): Production inference on GPUs for feature transforms and neural network execution.\n",
    "\n",
    "See you in the next lab!\n",
    "\n",
    "If you would like to see the GPU numbers drop to zero one more time, feel free to fun the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1105d",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
